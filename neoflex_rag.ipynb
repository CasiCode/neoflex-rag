{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JRjcVur4b3Vf"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import threading\n",
    "import json\n",
    "import re\n",
    "from typing_extensions import List\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langgraph.graph import MessagesState, StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "import requests\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import web_scrapers\n",
    "import constants\n",
    "import api\n",
    "\n",
    "import yaml\n",
    "from box import Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "config = Box(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I66rU8DZgJNE",
    "outputId": "51d203b8-a87c-4dca-cc14-c8382bd4e44e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter API key for OpenRouter: ··········\n"
     ]
    }
   ],
   "source": [
    "if not os.environ.get('OPENROUTER_API_KEY'):\n",
    "  os.environ['OPENROUTER_API_KEY'] = getpass.getpass('Enter API key for OpenRouter: ')\n",
    "\n",
    "if not os.environ.get('OPENROUTER_BASE_URL'):\n",
    "  os.environ['OPENROUTER_BASE_URL'] = 'https://openrouter.ai/api/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5b0u5TteLqG"
   },
   "source": [
    "В качестве LLM выбрана gpt-4.1-nano. Дешевая, быстрая модель, поддерживающая тулзы - это будет важно позже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F42zvbl-Oed8"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "  openai_api_key=os.environ.get('OPENROUTER_API_KEY'),\n",
    "  openai_api_base=os.environ.get('OPENROUTER_BASE_URL'),\n",
    "  model_name=config.model.name,\n",
    "  temperature=config.model.temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crWOCErzoeC8"
   },
   "source": [
    "Создаем локальное векторное хранилище, к нему подключаем эмбеддинги E5 (теперь от HuggingFace, ведь, как оказалось, Fastembeddings под капотом меняли E5 на другую модель). По идее, E5 должна хорошо подходить для смеси русского текста и английской терминологии, а еще показывает неплохие показатели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EZr8fi2a_FA6"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "embeddings = HuggingFaceEmbeddings(model_name=config.embeddings.model_name)\n",
    "vector_store = InMemoryVectorStore(embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvuExHvgfNUM"
   },
   "source": [
    "**UPD**: Используем crawler от Apify, у него есть интеграция с Langchain. Он пробегается по сайту от главной страницы вглубь и сохраняет информацию со всех страниц.\n",
    "\n",
    "Начинаем с главной страницы сайта, идем до глубины 3, стараемся преобразовывать html в читаемый текст, убираем порог у readableText, чтобы не пропускать даже мелкие вставки. Используем firefox браузер из playwright, чтобы читать больше информации с сайта - на нем куча javascript'а, который Crawl4AI (используемый ранее), обработать не мог.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Важно:**\n",
    "\n",
    "Для большей воспроизводимости и упрощения работы с этим блокнотом я вынес все, что связано с использованием Apify в отдельную ячейку.\n",
    "\n",
    "Вместо ввода API-токена и ожидания краулера можно запустить соседнюю ячейку, чтобы подгрузить те же данные, посчитанные мной заранее. Они хранятся у них на сервере и не требуют API-ключа для получения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsWOivLGF-cb"
   },
   "outputs": [],
   "source": [
    "from langchain_apify import ApifyWrapper\n",
    "\n",
    "\n",
    "if not os.environ.get('APIFY_API_TOKEN'):\n",
    "  os.environ['APIFY_API_TOKEN'] = getpass.getpass('Enter API token for Apify: ')\n",
    "\n",
    "apify = ApifyWrapper()\n",
    "\n",
    "loader = apify.call_actor(\n",
    "    actor_id='apify/website-content-crawler',\n",
    "    run_input={\n",
    "        'startUrls': [\n",
    "            {'url': 'https://www.neoflex.ru/'}\n",
    "            ],\n",
    "        'maxCrawlPages': config.apify.max_pages,\n",
    "        'maxCrawlDepth': config.apify.max_depth,\n",
    "        'htmlTransformer': 'readableTextIfPossible',\n",
    "        'readableTextCharThreshold': config.apify.threshold,\n",
    "        'crawlerType': 'playwright:firefox'\n",
    "        },\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item['text'] or '', metadata={'source': item['url']}\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e1YjUwapgpvP"
   },
   "outputs": [],
   "source": [
    "# Для подгрузки без API токена\n",
    "data = requests.get('https://api.apify.com/v2/datasets/oDw3TPSSsJ2dZ4ayz/items?clean=true&format=json')\n",
    "\n",
    "docs = []\n",
    "for item in data.json():\n",
    "    docs.append(\n",
    "        Document(\n",
    "            page_content=item['text'] or '', metadata={'source': item['url']}\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wB3QIt3s5p0"
   },
   "source": [
    "Делим полученные документы на чанки рекурсивным сплиттером:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FLOY4lK3Xuo9"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=config.documents.chunk_size,\n",
    "    chunk_overlap=config.documents.chunk_overlap\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUjbdSKAbDnA"
   },
   "source": [
    "Теперь займемся очищением документов от мусора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SF2rI4FSTd1"
   },
   "source": [
    "Пишем функцию для очистки документов от навигационных артефактов, не несущих смысла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TaH7VesZSs9M"
   },
   "outputs": [],
   "source": [
    "def clean_navigation_artifacts(text: str):\n",
    "    lines = text.splitlines()\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if not line:\n",
    "            continue\n",
    "        if re.fullmatch(r'20\\d{2}', line):\n",
    "            continue\n",
    "        if re.fullmatch(r'\\d{1,2}', line):\n",
    "            continue\n",
    "\n",
    "        if line.lower() in [\n",
    "            'previous', 'next', 'поделиться', 'отправить на e-mail', 'узнать'\n",
    "            'пресс-центр', 'новости', 'сми о нас', 'показать еще', '...',\n",
    "            'подписаться на новости', 'отправить', 'поделитьсяотправить на e-mail'\n",
    "        ]:\n",
    "            continue\n",
    "\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    return '\\n'.join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RZXnpmSTAVu"
   },
   "source": [
    "Прогоняем через нее все полученные чанки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "myfjnt39TFZK"
   },
   "outputs": [],
   "source": [
    "for split in all_splits:\n",
    "    split.page_content = clean_navigation_artifacts(split.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk_regV6NnyU"
   },
   "source": [
    "Прогоняем полученные чанки через TF-IDF фильтрацию, чтобы отсеять воду и мусор. В нашем случае удалим 30% документов, худшие по TF-IDF score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PloT-sV_zxa-"
   },
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in all_splits]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "scores = tfidf_matrix.sum(axis=1)\n",
    "scores = np.array(scores).flatten()\n",
    "\n",
    "threshold = np.percentile(scores, config.tfidf.threshold_percentile)\n",
    "\n",
    "filtered_docs = [\n",
    "    doc for doc, score in zip(all_splits, scores)\n",
    "    if score > threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtVoT2EpDGk2"
   },
   "source": [
    "Обогащаем полученный набор документов информацией, которую crawler от Apify не смог достать с сайта - контактами компании в разных городах, информацией о клиентах Neoflex и имейлом отдела кадров.\n",
    "\n",
    "О контактах и клиентах компании:\n",
    "\n",
    "- Контакты разных офисов и клиенты компании подгружаются динамически без перехода на новые страницы. Crawler не умеет работать с такими элементами\n",
    "- Я пытался найти API-запрос, по которому подгружается нужная информация, чтобы обогатить документы из него, но, как оказалось, все адреса захардкожены в обычный массив внутри vue.js скрипта, а информация о клиентах лежит частично хардкодом в DOM, а частично где-то во vue.js коде\n",
    "- Вместо попыток достать информацию с бэкенда я решил написать скрэппер для этих страниц, который будет прокликивать кнопки и собирать информацию вручную. Да, это долго, но это работает\n",
    "\n",
    "Об имейле отдела кадров:\n",
    "\n",
    "- Как и с адресами офисов, при тестах выяснилось, что нет документов о подходящем для отправки резюме имейле.\n",
    "- Эта информация был отсеяна crawler'ом на этапе преобразования html в текст, так как он счел ее нерелевантной\n",
    "- Также достанем ее отдельным маленьким скрэппером"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PvM4Rf_e8Lp"
   },
   "source": [
    "Собирать информацию будем скриптами на playwright из файла web_scrapers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v5KzVjHbq6o"
   },
   "source": [
    "Применяем скрэпперы и заливаем информацию из них в общий массив документов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "R6MjCnOC-hSO"
   },
   "outputs": [],
   "source": [
    "city_data = await web_scrapers.scrape_city_addresses()\n",
    "\n",
    "city_docs = [\n",
    "    Document(\n",
    "        metadata={'source': constants.CONTACTS_URL},\n",
    "        page_content=f'Контакты офисов компании в городе {name} (адрес, электронная почта, телефон): {data}'\n",
    "    )\n",
    "    for name, data in city_data.items()\n",
    "]\n",
    "\n",
    "customer_data = await web_scrapers.scrape_customer_details()\n",
    "customer_docs = [\n",
    "    Document(\n",
    "        metadata={'source': constants.CUSTOMERS_URL},\n",
    "        page_content=f'Информация об одном из клиентов (заказчиков) компании Neoflex: {data}'\n",
    "    )\n",
    "    for data in customer_data\n",
    "]\n",
    "\n",
    "career_doc = Document(\n",
    "    metadata={'source': constants.CAREER_URL},\n",
    "    page_content=(await web_scrapers.scrape_career_details())\n",
    ")\n",
    "\n",
    "filtered_docs += city_docs\n",
    "filtered_docs += customer_docs\n",
    "filtered_docs.append(career_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgbCO00SWdM-"
   },
   "source": [
    "Теперь добавим теги ко всем документам для будущего поиска по ним. Ориентироваться будем на url страницы, породившей документ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vToBWj0-WcX9"
   },
   "outputs": [],
   "source": [
    "tags = {\n",
    "    'Решения': '/solutions',\n",
    "    'Кейсы': '/project-list',\n",
    "    'Экспертизы': '/expertises',\n",
    "    'О компании': '/about',\n",
    "    'Партнеры': '/about/partners',\n",
    "    'Клиенты': '/about/customers',\n",
    "    'Карьера': '/about/career',\n",
    "    'Пресс-центр': '/press-center',\n",
    "    'Контакты': '/contacts',\n",
    "}\n",
    "\n",
    "for doc in filtered_docs:\n",
    "    matched_tags = [\n",
    "        tag for tag, snippet in tags.items()\n",
    "        if snippet and snippet in doc.metadata.get('source', '')\n",
    "    ]\n",
    "    doc.metadata.setdefault('tags', []).extend(matched_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzKo-tNtcGvN"
   },
   "source": [
    "Кроме этих тегов в теории можно добавить, например, теги городов (\"Саратов\", \"Москва\" и т.д.), сфер деятельности (\"MLops\", \"Мобильная разработка\") или компаний (\"Сбер\", \"Россельхоз\" и т.д.) парсингом page_content докуменов или прогнав их содержимое через LLM, подбирающее теги (такой подход будет использован для извлечения тегов из пользовательских запросов далее)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBN5xz06WcKa"
   },
   "source": [
    "Наконец, добавляем обязательный префикс для E5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EgblEeUqWTZD"
   },
   "outputs": [],
   "source": [
    "for doc in filtered_docs:\n",
    "    doc.page_content = f'passage: {doc.page_content}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkgaMs4ieECc"
   },
   "source": [
    "Выведем 5 рандомных чанков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "srhwAKdiX8Di",
    "outputId": "7312cb7e-6108-47d3-dd72-1166c85547b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata: {'source': 'https://www.neoflex.ru/projects/tsifrovye-bankovskie-produkty-i-servisy-dlya-msb', 'tags': []}\n",
      "passage: Клиентам банка также доступны различные нефинансовые сервисы в электронном формате: регистрация бизнеса, онлайн-бухгалтерия, проверка контрагентов, привлечение клиентов, налоговое консультирование и юридическая поддержка. В ближайших планах – внедрение кредитного продукта в форме овердрафта.\n",
      "Отзыв заказчика\n",
      "Анатолий Медведев\n",
      "Начальник управления малого и среднего бизнеса «Ренессанс Банка»\n",
      "Предприниматели в «Ренессанс Банке» обслуживаются исключительно в цифровых каналах. Точка входа – сайт, там простая форма заявки на открытие счета. Развивая это направление бизнеса, мы сознательно идем по цифровому сценарию, потому что это отвечает запросам современных предпринимателей. Мы видим своей первостепенной задачей сделать дистанционное обслуживание максимально комфортным, надежным и удобным.\n",
      "Новости и СМИ о проекте\n",
      "«Ренессанс Банк» при поддержке Neoflex запустил цифровые банковские продукты и сервисы для малого и среднего бизнеса\n",
      "18 января 2024\n",
      "\n",
      "\n",
      "metadata: {'source': 'https://www.neoflex.ru/solutions/transportation-management', 'tags': ['Решения']}\n",
      "passage: Neoflex — Решения — Управление перевозками\n",
      "Интеграция приложений\n",
      "Управление перевозками\n",
      "Платформа потоковой обработки событий ИБ\n",
      "Система для управления эффективностью грузоперевозок на базе технологий Big Data и Fast Data\n",
      "Преимущества\n",
      "Гибкий конструктор для описания модели логистики компании\n",
      "Интеллектуальное планирование перевозок с использованием методов машинного обучения\n",
      "Прогноз остатков на складах на месяц вперед с детализацией до каждого груза\n",
      "Потоковая аналитика и предиктивное выявление потенциальных инцидентов\n",
      "Интеграция данных телеметрии и информации о пробках на дорогах\n",
      "Специализированный конструктор позволяет разрабатывать и сопровождать модель логистики компании на уровне правил и шаблонов.\n",
      "В решении реализован расчет прогноза складских остатков с детализаций до отдельного груза в режиме реального времени. Скорость и уровень детализации обеспечивается за счет применения технологий in-memory и потоковых вычислений.\n",
      "\n",
      "\n",
      "metadata: {'source': 'https://www.neoflex.ru/news/kak-obespechit-bezopasnost-oblachnoy-infrastruktury-', 'tags': []}\n",
      "passage: Neoflex принял участие в вебинаре Yandex Cloud «Векторы атак на облачную инфраструктуру и подходы к защите». Эксперты рассказали о том, какие существуют атаки на облачные сервисы, а также рассмотрели подходы и инструменты, которые помогут избежать проникновения злоумышленника в облачную инфраструктуру. Евгений Кондратьев, менеджер продуктов Neoflex, представил решение NeoCAT (Neoflex Cloud Assessment Tool) для автоматизации процессов безопасности в облаке. Продукт будет особенно эффективен для компаний среднего и крупного бизнеса с активно развивающейся, либо изменяющейся облачной инфраструктурой. В частности, спикер отметил, что NeoCAT позволяет оценить, насколько безопасно сконфигурирована облачная инфраструктура, а также сервисы и приложения компании. Решение сканирует информацию о конфигурации ресурсов в облаке через API самого облака, заблаговременно выявляет риски в соответствие со стандартами безопасности, помогает их устранить и предотвратить появление в будущем. Продукт\n",
      "\n",
      "\n",
      "metadata: {'source': 'https://www.neoflex.ru/news/neoflex-i-timeweb-cloud-obyavili-o-nachale-strategicheskogo-partnerstva-', 'tags': []}\n",
      "passage: Кроме того, продукты компаний входят в реестр отечественного ПО, что дополнительно позволит решать задачи, связанные с импортозамещением зарубежных облачных сервисов и контейнерных платформ Kubernetes. В дальнейшем компании планируют возможность перекрестной интеграции еще более широкого спектра сервисов в рамках обеих платформ.\n",
      "«Мы наблюдаем все больше индивидуальных запросов на перенос On-Premise инфраструктуры в облака. И сейчас одна из наших задач — обеспечить быстрое и финансово эффективное решение для каждого клиента, в том числе для представителей малого, среднего и крупного бизнеса. Поэтому партнерство с одним из ведущих российских ИТ-интеграторов — стратегический и своевременный шаг в развитии нашего облака», – прокомментировал Сергей Наумов, генеральный директор Timeweb Cloud.\n",
      "\n",
      "\n",
      "metadata: {'source': 'https://www.neoflex.ru/publications/na-rynke-seychas-vzryv-dannykh-my-staraemsya-uchest-vsye-chto-vliyaet-na-media-intervyu-vasiliya-kuz', 'tags': []}\n",
      "passage: Также мы выработали процесс расширения этой модели, и сейчас вовлекаем партнеров в процесс ее развития. До конца прошлого года мы пополнили данными все, что есть в модели, эти же данные подключили к платформе и запустили решение в промышленную эксплуатацию, что коснулось, в первую очередь, телевизионных рейтингов, которые рассчитываются на ежедневной основе.\n",
      "CNews: Вы много раз упоминали развитие платформы. А каким оно будет?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "for _ in range(5):\n",
    "    idx = randrange(len(filtered_docs)-1)\n",
    "    print(f'metadata: {filtered_docs[idx].metadata}')\n",
    "    print(f'{filtered_docs[idx].page_content}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiMseV6rr-wS"
   },
   "source": [
    "Загружаем все внутрь VectorStore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "HzrSxT_ZfoZC"
   },
   "outputs": [],
   "source": [
    "document_ids = vector_store.add_documents(documents=filtered_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTSIUCJShGYU"
   },
   "source": [
    "Пишем функцию get_query_tags: она будет обращаться к LLM, чтобы извлечь подходящие теги из пользовательского запроса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ka5ZXvucvwbe"
   },
   "outputs": [],
   "source": [
    "def get_query_tags(user_query: str):\n",
    "    prompt = load_prompt('prompts/tag-getter-prompt.txt')\n",
    "    f_prompt = prompt.format(user_query=user_query)\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    try:\n",
    "        tags = eval(response.content.strip())\n",
    "    except:\n",
    "        tags = []\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRGgD49GdR76"
   },
   "source": [
    "Пишем функцию reformulate_query, которая обращается к LLM вне контекста, чтобы та переформулировала запрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VsmYfM_7dSTY"
   },
   "outputs": [],
   "source": [
    "def reformulate_query(user_query: str):\n",
    "    user_query = user_query[:config.reformulate.max_length]\n",
    "\n",
    "    prompt = [\n",
    "        SystemMessage(content=(\n",
    "            load_prompt('prompts/reformulation-prompt.txt')\n",
    "        )),\n",
    "        HumanMessage(content=user_query)\n",
    "    ]\n",
    "    response = llm.invoke(prompt)\n",
    "    try:\n",
    "        return response.content.strip() or user_query\n",
    "    except:\n",
    "        return user_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxj3fdDyVm2p"
   },
   "source": [
    "Пишем функцию normalize_similarity_score. Она позволит удобнее и интуитивнее работать со скорами семантического поиска, нормализуя их на интервал [0, 1]. Это сильно удобнее, чем компактный интервал [0.7, 1], в котором по умолчанию лежат скоры для E5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zma3UyIfViCJ"
   },
   "outputs": [],
   "source": [
    "def normalize_similarity_score(scored_docs, lo, hi):\n",
    "    result = [\n",
    "        (doc, (score - lo) / (hi - lo))\n",
    "        for doc, score in scored_docs\n",
    "    ]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wdL0-SRWH_c"
   },
   "source": [
    "Пишем функцию soft_merge. Ее задача - \"мягко\" объединить два набора документов. Удаляем из наборов все документы, у которых скор хуже среднего, а потом соединяем наборы вместе, сортируем и возвращаем K лучших документов, где K - длина наибольшего набора.\n",
    "\n",
    "Если какие-то документы окажутся настолько релевантными, что перетянут в свою сторону средний скор, то функция может вернуть меньше, чем K документов. Это поведение ожидаемо - идея в том, что вылетевшие документы в таком случае не будут мешать модели работать с самыми релевантыми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8-zCs-pnWHa2"
   },
   "outputs": [],
   "source": [
    "def soft_merge(lhs_docs, rhs_docs):\n",
    "    res_len = max(len(lhs_docs), len(rhs_docs))\n",
    "\n",
    "    lhs_docs = normalize_similarity_score(\n",
    "        lhs_docs, config.embeddings.score_lo, config.embeddings.score_hi)\n",
    "    rhs_docs = normalize_similarity_score(\n",
    "        rhs_docs, config.embeddings.score_lo, config.embeddings.score_hi)\n",
    "\n",
    "    lhs_mean = sum([score for _, score in lhs_docs]) / len(lhs_docs) if lhs_docs else 0\n",
    "    rhs_mean = sum([score for _, score in rhs_docs]) / len(rhs_docs) if rhs_docs else 0\n",
    "    lhs_docs = [(doc, score) for doc, score in lhs_docs if score >= lhs_mean]\n",
    "    rhs_docs = [(doc, score) for doc, score in rhs_docs if score >= rhs_mean]\n",
    "\n",
    "    docs = sorted(lhs_docs+rhs_docs, key=lambda x: x[1], reverse=True)\n",
    "    unique_docs = {doc.page_content: (doc, score) for doc, score in docs}.values()\n",
    "    unique_docs = list(unique_docs)\n",
    "\n",
    "    return unique_docs[:res_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0NmzE40XsNi"
   },
   "source": [
    "Объявляем функцию rerank_by_tags. Она принимает на вход массив документов, сравнивает их теги с целевыми и подтягивает скор докуметов в зависимости от количества совпадений. Возвращает отсортированный по скорам массив документов. При filter_irrelevant = True отсекает явный мусор.\n",
    "\n",
    "Затем, наконец, пишем функцию retrieve_from_local. Она, используя все объявленные ранее функции для поиска и ранжирования документов, ищет релевантную информацию в локальном хранилище:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "NpU6jQHodo6z"
   },
   "outputs": [],
   "source": [
    "def rerank_by_tags(\n",
    "    docs, target_tags, boost=config.rerank.boost,\n",
    "    filter_irrelevant=False, threshold=config.rerank.threshold\n",
    "):\n",
    "    reranked = []\n",
    "\n",
    "    if not target_tags:\n",
    "        reranked = docs\n",
    "    else:\n",
    "        target_tags = {t.lower() for t in target_tags if isinstance(t, str)}\n",
    "        for doc, score in docs:\n",
    "            doc_tags = set(doc.metadata.get('tags', []))\n",
    "            doc_tags_lo = {t.lower() for t in doc_tags}\n",
    "\n",
    "            matches = len(doc_tags_lo & target_tags)\n",
    "            adjusted_score = max(0.0, score + boost * matches)\n",
    "            reranked.append((doc, adjusted_score))\n",
    "\n",
    "    reranked_normalized = normalize_similarity_score(\n",
    "        reranked, config.embeddings.score_lo, config.embeddings.score_hi)\n",
    "    reranked_normalized.sort(key=lambda x: x[1], reverse=True)\n",
    "    reranked.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if filter_irrelevant:\n",
    "        reranked = [\n",
    "            (doc, score)\n",
    "            for doc, score in reranked if score > threshold\n",
    "            ]\n",
    "\n",
    "    return reranked # Invalid return, reranked_normilized is being forgotten\n",
    "\n",
    "\n",
    "@tool(response_format='content_and_artifact')\n",
    "def retrieve_from_local(query: str):\n",
    "    '''\n",
    "    Используй этот инструмент для поиска актуальной, специфической информации\n",
    "    о компании Neoflex в локальной базе знаний.\n",
    "\n",
    "    Вызывай этот инструмент для вопросов о:\n",
    "    - Адресах, местоположении офисов, контактных данных\n",
    "    - Проектах, решениях, заказчиках, технологиях, платформах, партнерах\n",
    "    - Конкретных фактах, датах, названиях, именах, ссылках и т.д.\n",
    "    - Любых других специфических деталях операций или структуры Neoflex.\n",
    "\n",
    "    Отдавай приоритет использованию этого инструмента перед своими внутренними знаниями\n",
    "    для проверки специфических фактов, чтобы гарантировать точность.\n",
    "\n",
    "    Attributes:\n",
    "    query (str): строковый запрос на естественном языке,\n",
    "        должен содержать ключевые слова для поиска.\n",
    "    '''\n",
    "\n",
    "    tags_set = set(tags.keys())\n",
    "    found_tags = set.intersection(set(get_query_tags(query)), tags_set)\n",
    "\n",
    "    r_query = reformulate_query(query)\n",
    "\n",
    "    r_retrieved_docs = vector_store.similarity_search_with_score(\n",
    "        f'query: {r_query}',\n",
    "        k=config.semantic_search.k\n",
    "    )\n",
    "    retrieved_docs = vector_store.similarity_search_with_score(\n",
    "        f'query: {query}',\n",
    "        k=config.semantic_search.k\n",
    "    )\n",
    "\n",
    "    docs = soft_merge(retrieved_docs, r_retrieved_docs)\n",
    "    reranked_docs = rerank_by_tags(docs, found_tags)\n",
    "\n",
    "    serialized = '\\n\\n'.join(\n",
    "        (f'Источник: {doc.metadata.get(\"url\", \"Неизвестный источник\")}\\n' f'Содержимое документа: {doc.page_content}')\n",
    "        for doc, _ in reranked_docs\n",
    "    )\n",
    "    return serialized, reranked_docs\n",
    "\n",
    "web_search = DuckDuckGoSearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqpKNsNvhY9t"
   },
   "source": [
    "Создаем будущие ноды графа:\n",
    "\n",
    "Первая будет отвечать за поиск подходящей информации либо прямой ответ (для случаев, когда разговор идет не по теме)\n",
    "\n",
    "Вторая - это ToolNode, к которому будет происходить обращение из первой ноды по необходимости\n",
    "\n",
    "Третья - гененирует итоговый ответ по результатам обращения к тулзам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "VEhSH4M1dRAW"
   },
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    context: List[Document]\n",
    "\n",
    "\n",
    "def query_or_respond(state: MessagesState):\n",
    "    prompt = [SystemMessage(content=load_prompt('prompts/qr-prompt')] + state['messages']\n",
    "    llm_with_tools = llm.bind_tools([retrieve_from_local, web_search])\n",
    "    response = llm_with_tools.invoke(prompt)\n",
    "    return {'messages': [response]}\n",
    "\n",
    "\n",
    "tools = ToolNode([retrieve_from_local, web_search])\n",
    "\n",
    "\n",
    "def generate(state: MessagesState):\n",
    "    recent_tool_msgs = []\n",
    "    for message in reversed(state['messages']):\n",
    "        if message.type == 'tool':\n",
    "            recent_tool_msgs.append(message)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    tool_msgs = recent_tool_msgs[::-1]\n",
    "\n",
    "    docs_content = '\\n\\n'.join(\n",
    "        doc.page_content\n",
    "        for tool_msg in tool_msgs\n",
    "        if tool_msg.artifact\n",
    "        for doc, _ in tool_msg.artifact\n",
    "    )\n",
    "    if not docs_content:\n",
    "        docs_content = 'Контекст отсутствует'\n",
    "    system_message_content = (\n",
    "        f\"{load_prompt('prompts/generate-prompt.txt')}\"\n",
    "        f'{docs_content}'\n",
    "    )\n",
    "    conversation_msgs = [\n",
    "        message\n",
    "        for message in state['messages']\n",
    "        if message.type in ('human', 'system')\n",
    "        or (message.type == 'ai' and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_msgs\n",
    "    # may be a good idea to add prefixes to previous msgs or some introduction line\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    context = []\n",
    "\n",
    "    for tool_msg in tool_msgs:\n",
    "        if tool_msg.artifact is not None:\n",
    "            context.extend(tool_msg.artifact)\n",
    "\n",
    "    return {'messages': [response], 'context': context}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgmBNyKgiFnz"
   },
   "source": [
    "Создаем граф, добавляем в него ноды, настраиваем ребра, добавляем ему память, чтобы RAG помнил контекст разговора, собираем все в кучу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "aoi5IxkMdX2w"
   },
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point('query_or_respond')\n",
    "graph_builder.add_conditional_edges(\n",
    "    'query_or_respond',\n",
    "    tools_condition,\n",
    "    {END: END, 'tools': 'tools'}\n",
    ")\n",
    "\n",
    "graph_builder.add_edge('tools', 'generate')\n",
    "graph_builder.add_edge('generate', END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "config = {'configurable': {'thread_id': 'abc123'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zw5H8q-aSFsA"
   },
   "source": [
    "Далее пишем функцию, обрабатывающую входящее сообщение и дающую на него ответ в требуемом по ТЗ формате. Возвращаем источники, как для поика по локальной базе, так и для поиска с помощью DuckDuckGo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "jaZO9VeQe7aC"
   },
   "outputs": [],
   "source": [
    "def process_input_message(session_id: str, input_message: str):\n",
    "    response = graph.invoke(\n",
    "        {'messages': [{'role': 'user', 'content': input_message}], 'context': []},\n",
    "        stream_mode='values',\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    src = []\n",
    "    if response.get('context') and len(response['context']) > 0:\n",
    "        for doc in response['context']:\n",
    "            if isinstance(doc, dict):\n",
    "                src.append({\n",
    "                    'source': doc['link'],\n",
    "                    'snippet': doc['snippet']\n",
    "                })\n",
    "            if isinstance(doc, tuple):\n",
    "                src.append({\n",
    "                    'source': doc[0].metadata.get('source', 'unknown'),\n",
    "                    'snippet': f'relevance: {doc[1]} ' + doc[0].page_content[:150] + '...'\n",
    "                    # добавил скор в вывод для наглядности\n",
    "                })\n",
    "            else: src.append({\n",
    "                'source': 'unknown',\n",
    "                'snippet': f'Были получены неожиданные результаты поиска: {type(doc)}'\n",
    "                })\n",
    "\n",
    "    return {\n",
    "        'answer': response['messages'][-1].content\n",
    "        if response.get('messages') else \"Ответ не получен.\",\n",
    "        'source_documents': src,\n",
    "        'session_id': session_id\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEaEYghNpQwI"
   },
   "source": [
    "Создаем инстанс FastAPI и три класса, наследующих от pydantic BaseModel - с их помощью будем следить за структурой генерируемых запросов и ответов от API.\n",
    "\n",
    "Пишем функцию ask_question с декоратором app.post('/ask, ...). С его помощью будем ловить POST запросы к API и обрабатывать их, возвращая ответ.\n",
    "\n",
    "В отдельном потоке запускаем локальный сервер на uvicorn'е с нашей API-шкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "hN2-AHVjcgnZ"
   },
   "outputs": [],
   "source": [
    "api.set_process_function(process_input_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85qhokOsqS_B"
   },
   "source": [
    "Запускаем локальный сервер на uvicorn'е, пишем будущий json запрос, передаем его POST запросом на сервер, получаем ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JvBJfyP8jwtz",
    "outputId": "4ec2b212-4396-4bf0-fbdf-b3c8b48758a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"answer\": \"Neoflex обладает экспертизой в области Site Reliability Engineering, DevOps, разработки Data-платформ, MLOps, трансформации приложений и инструментов, а также в создании платформ для обработки и хранения данных.\",\n",
      "    \"source_documents\": [\n",
      "        {\n",
      "            \"source\": \"https://www.neoflex.ru/expertises/sre\",\n",
      "            \"snippet\": \"relevance: 0.6709694879472464 passage: Neoflex — Экспертиза — Site Reliability Engineering\\nDevOps\\nSite Reliability Engineering\\nРазработка Data-платформ\\nТрансформация приложений, ин...\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"https://www.neoflex.ru/expertises/big-data\",\n",
      "            \"snippet\": \"relevance: 0.6659197877406068 passage: Neoflex — Экспертиза — Разработка Data-платформ\\nSite Reliability Engineering\\nРазработка Data-платформ\\nMLOps\\nСоздаем платформы для обработки и...\"\n",
      "        }\n",
      "    ],\n",
      "    \"session_id\": \"abc123\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    'session_id': 'abc123',\n",
    "    'question': 'В каких областях Neoflex обладает экспертизой?'\n",
    "}\n",
    "\n",
    "response = requests.post('http://127.0.0.1:8000/ask', json=query)\n",
    "print(json.dumps(response.json(), indent=4, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
