{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JRjcVur4b3Vf"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import web_scrapers\n",
    "import constants\n",
    "from vectorstore import vector_store\n",
    "\n",
    "from utils import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvuExHvgfNUM"
   },
   "source": [
    "**UPD**: Используем crawler от Apify, у него есть интеграция с Langchain. Он пробегается по сайту от главной страницы вглубь и сохраняет информацию со всех страниц.\n",
    "\n",
    "Начинаем с главной страницы сайта, идем до глубины 3, стараемся преобразовывать html в читаемый текст, убираем порог у readableText, чтобы не пропускать даже мелкие вставки. Используем firefox браузер из playwright, чтобы читать больше информации с сайта - на нем куча javascript'а, который Crawl4AI (используемый ранее), обработать не мог.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Важно:**\n",
    "\n",
    "Для большей воспроизводимости и упрощения работы с этим блокнотом я вынес все, что связано с использованием Apify в отдельную ячейку.\n",
    "\n",
    "Вместо ввода API-токена и ожидания краулера можно запустить соседнюю ячейку, чтобы подгрузить те же данные, посчитанные мной заранее. Они хранятся у них на сервере и не требуют API-ключа для получения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsWOivLGF-cb"
   },
   "outputs": [],
   "source": [
    "from langchain_apify import ApifyWrapper\n",
    "\n",
    "\n",
    "if not os.environ.get('APIFY_API_TOKEN'):\n",
    "  os.environ['APIFY_API_TOKEN'] = getpass.getpass('Enter API token for Apify: ')\n",
    "\n",
    "apify = ApifyWrapper()\n",
    "\n",
    "loader = apify.call_actor(\n",
    "    actor_id='apify/website-content-crawler',\n",
    "    run_input={\n",
    "        'startUrls': [\n",
    "            {'url': 'https://www.neoflex.ru/'}\n",
    "            ],\n",
    "        'maxCrawlPages': config.apify.max_pages,\n",
    "        'maxCrawlDepth': config.apify.max_depth,\n",
    "        'htmlTransformer': 'readableTextIfPossible',\n",
    "        'readableTextCharThreshold': config.apify.threshold,\n",
    "        'crawlerType': 'playwright:firefox'\n",
    "        },\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item['text'] or '', metadata={'source': item['url']}\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e1YjUwapgpvP"
   },
   "outputs": [],
   "source": [
    "# Для подгрузки без API токена\n",
    "data = requests.get('https://api.apify.com/v2/datasets/oDw3TPSSsJ2dZ4ayz/items?clean=true&format=json')\n",
    "\n",
    "docs = []\n",
    "for item in data.json():\n",
    "    docs.append(\n",
    "        Document(\n",
    "            page_content=item['text'] or '', metadata={'source': item['url']}\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wB3QIt3s5p0"
   },
   "source": [
    "Делим полученные документы на чанки рекурсивным сплиттером:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FLOY4lK3Xuo9"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=config.documents.chunk_size,\n",
    "    chunk_overlap=config.documents.chunk_overlap\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUjbdSKAbDnA"
   },
   "source": [
    "Теперь займемся очищением документов от мусора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SF2rI4FSTd1"
   },
   "source": [
    "Пишем функцию для очистки документов от навигационных артефактов, не несущих смысла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TaH7VesZSs9M"
   },
   "outputs": [],
   "source": [
    "def clean_navigation_artifacts(text: str):\n",
    "    lines = text.splitlines()\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if not line:\n",
    "            continue\n",
    "        if re.fullmatch(r'20\\d{2}', line):\n",
    "            continue\n",
    "        if re.fullmatch(r'\\d{1,2}', line):\n",
    "            continue\n",
    "\n",
    "        if line.lower() in [\n",
    "            'previous', 'next', 'поделиться', 'отправить на e-mail', 'узнать'\n",
    "            'пресс-центр', 'новости', 'сми о нас', 'показать еще', '...',\n",
    "            'подписаться на новости', 'отправить', 'поделитьсяотправить на e-mail'\n",
    "        ]:\n",
    "            continue\n",
    "\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    return '\\n'.join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RZXnpmSTAVu"
   },
   "source": [
    "Прогоняем через нее все полученные чанки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "myfjnt39TFZK"
   },
   "outputs": [],
   "source": [
    "for split in all_splits:\n",
    "    split.page_content = clean_navigation_artifacts(split.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk_regV6NnyU"
   },
   "source": [
    "Прогоняем полученные чанки через TF-IDF фильтрацию, чтобы отсеять воду и мусор. В нашем случае удалим 30% документов, худшие по TF-IDF score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PloT-sV_zxa-"
   },
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in all_splits]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "scores = tfidf_matrix.sum(axis=1)\n",
    "scores = np.array(scores).flatten()\n",
    "\n",
    "threshold = np.percentile(scores, config.tfidf.threshold_percentile)\n",
    "\n",
    "filtered_docs = [\n",
    "    doc for doc, score in zip(all_splits, scores)\n",
    "    if score > threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtVoT2EpDGk2"
   },
   "source": [
    "Обогащаем полученный набор документов информацией, которую crawler от Apify не смог достать с сайта - контактами компании в разных городах, информацией о клиентах Neoflex и имейлом отдела кадров.\n",
    "\n",
    "О контактах и клиентах компании:\n",
    "\n",
    "- Контакты разных офисов и клиенты компании подгружаются динамически без перехода на новые страницы. Crawler не умеет работать с такими элементами\n",
    "- Я пытался найти API-запрос, по которому подгружается нужная информация, чтобы обогатить документы из него, но, как оказалось, все адреса захардкожены в обычный массив внутри vue.js скрипта, а информация о клиентах лежит частично хардкодом в DOM, а частично где-то во vue.js коде\n",
    "- Вместо попыток достать информацию с бэкенда я решил написать скрэппер для этих страниц, который будет прокликивать кнопки и собирать информацию вручную. Да, это долго, но это работает\n",
    "\n",
    "Об имейле отдела кадров:\n",
    "\n",
    "- Как и с адресами офисов, при тестах выяснилось, что нет документов о подходящем для отправки резюме имейле.\n",
    "- Эта информация был отсеяна crawler'ом на этапе преобразования html в текст, так как он счел ее нерелевантной\n",
    "- Также достанем ее отдельным маленьким скрэппером"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PvM4Rf_e8Lp"
   },
   "source": [
    "Собирать информацию будем скриптами на playwright из файла web_scrapers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v5KzVjHbq6o"
   },
   "source": [
    "Применяем скрэпперы и заливаем информацию из них в общий массив документов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6MjCnOC-hSO"
   },
   "outputs": [],
   "source": [
    "city_data = await web_scrapers.scrape_city_addresses()\n",
    "\n",
    "city_docs = [\n",
    "    Document(\n",
    "        metadata={'source': constants.CONTACTS_URL},\n",
    "        page_content=f'Контакты офисов компании в городе {name} (адрес, электронная почта, телефон): {data}'\n",
    "    )\n",
    "    for name, data in city_data.items()\n",
    "]\n",
    "\n",
    "customer_data = await web_scrapers.scrape_customer_details()\n",
    "customer_docs = [\n",
    "    Document(\n",
    "        metadata={'source': constants.CUSTOMERS_URL},\n",
    "        page_content=f'Информация об одном из клиентов (заказчиков) компании Neoflex: {data}'\n",
    "    )\n",
    "    for data in customer_data\n",
    "]\n",
    "\n",
    "career_doc = Document(\n",
    "    metadata={'source': constants.CAREER_URL},\n",
    "    page_content=(await web_scrapers.scrape_career_details())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_docs += city_docs\n",
    "filtered_docs += customer_docs\n",
    "filtered_docs.append(career_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgbCO00SWdM-"
   },
   "source": [
    "Теперь добавим теги ко всем документам для будущего поиска по ним. Ориентироваться будем на url страницы, породившей документ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vToBWj0-WcX9"
   },
   "outputs": [],
   "source": [
    "from constants import DOCUMENT_TAGS\n",
    "\n",
    "for doc in filtered_docs:\n",
    "    matched_tags = [\n",
    "        tag for tag, snippet in DOCUMENT_TAGS.items()\n",
    "        if snippet and snippet in doc.metadata.get('source', '')\n",
    "    ]\n",
    "    tagline = ' '.join(matched_tags) or ''\n",
    "    doc.metadata['tags'] = tagline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzKo-tNtcGvN"
   },
   "source": [
    "Кроме этих тегов в теории можно добавить, например, теги городов (\"Саратов\", \"Москва\" и т.д.), сфер деятельности (\"MLops\", \"Мобильная разработка\") или компаний (\"Сбер\", \"Россельхоз\" и т.д.) парсингом page_content докуменов или прогнав их содержимое через LLM, подбирающее теги (такой подход будет использован для извлечения тегов из пользовательских запросов далее)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBN5xz06WcKa"
   },
   "source": [
    "Наконец, добавляем обязательный префикс для E5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EgblEeUqWTZD"
   },
   "outputs": [],
   "source": [
    "for doc in filtered_docs:\n",
    "    doc.page_content = f'passage: {doc.page_content}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkgaMs4ieECc"
   },
   "source": [
    "Выведем 5 рандомных чанков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "srhwAKdiX8Di",
    "outputId": "7312cb7e-6108-47d3-dd72-1166c85547b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata: {'source': 'https://www.neoflex.ru/about/customers', 'tags': 'О компании Клиенты'}\n",
      "passage: Информация об одном из клиентов (заказчиков) компании Neoflex: Проекты Расчет резервов на ожидаемые кредитные убытки по МСФО (IFRS) 9 для ПАО «БАНК УРАЛСИБ» на базе системы Finastra Fusion Risk и интеграцией через Informatica PowerCenter Комплексный ИТ-аудит омниканальной микросервисной платформы Новости Банк Уралсиб и Neoflex успешно завершили аудит омниканальной платформы\n",
      "\n",
      "\n",
      "metadata: {'source': 'https://www.neoflex.ru/publications/big-data-menyaet-protsess-postroeniya-otchetnosti', 'tags': ''}\n",
      "passage: Полную версию публикации читайте на сайте издания NBJ.\n",
      "Neoflex MSA Platform — видимая и подводная части айсберга. Статья Лины Чудновой и Юрия Корнвейца для журнала «Банковское обозрение»DevOps меняет процесс грузоперевозок. Интервью Антона Бечина для портала CNews\n",
      "Контакты для МЕДИА\n",
      "Если у вас есть вопросы или нужна дополнительная информация о компании, напишите нам по адресу пресс-службы:\n",
      "pr@neoflex.ru\n",
      "\n",
      "\n",
      "metadata: {'source': 'https://www.neoflex.ru/publications/migration-hadoop-habr', 'tags': ''}\n",
      "passage: файлы, с которыми легко работать, перемещать, бэкапить и реплицировать;\n",
      "колончатый вид позволяет значительно ускорить работу аналитика;\n",
      "нативная поддержка в Spark из коробки обеспечивает возможность сохранить файл в любимое хранилище.\n",
      "Подробности читайте в статье\n",
      "Кредитный конвейер: на какие решения ориентироваться банкам. Мнение экспертов NeoflexLightbend Cloudflow - hазработка конвейеров потоковой обработки данных. Статья Антона Алексеева для Хабр\n",
      "Контакты для МЕДИА\n",
      "Если у вас есть вопросы или нужна дополнительная информация о компании, напишите нам по адресу пресс-службы:\n",
      "pr@neoflex.ru\n",
      "\n",
      "\n",
      "metadata: {'source': 'https://www.neoflex.ru/expertises/devops', 'tags': 'Экспертизы'}\n",
      "passage: DevOps Cloud\n",
      "Непрерывная интеграция и доставка\n",
      "Автоматизация развертывания серверов\n",
      "Настройка мониторинга инфраструктуры\n",
      "Почему Neoflex\n",
      "Внедрение CI/CD для различных платформ: монолитных и микросервисных приложений под управлением Kubernetes\n",
      "Time to Market и качество приложений. Быстрый ввод в эксплуатацию новой функциональности без потери качества за счёт автоматизации конвейера разработки, включая развертывание сред, тестирование, выпуск релизов\n",
      "Снижение хрупкости бизнес-критичных систем за счёт построения High Availability и Disaster Recovery топологий инфраструктуры и применения архитектурных паттернов CQRS, Saga, Event Sourcing\n",
      "Снижение стоимости внедрения DevOps-платформы и разработки за счет использования готовых архитектурных паттернов, шаблонов инфраструктурных компонентов и автоматизированных производственных процессов\n",
      "Эффективный мониторинг за счет проактивного реагирования и прогнозирования сбоев\n",
      "Кейсы\n",
      "Новости и СМИ по теме\n",
      "\n",
      "\n",
      "metadata: {'source': 'https://www.neoflex.ru/publications/fast-data-online-streaming-processing-cnews', 'tags': ''}\n",
      "passage: Бизнес хочет быстрые данные вместо больших и уходит в онлайн-обработку. Интервью Геннадия Волкова и Лины Чудновой для CNews.ru\n",
      "Пакетная обработка данных стала слишком медленной и не удовлетворяет потребностям бизнеса. На российский рынок пришли решения класса Fast Data, основанные на потоковой обработке и использующие методы машинного обучения. О том, как большие данные становятся быстрыми и какую практическую пользу смогут извлечь из этого отечественные компании, в совместном интервью CNews рассказали Геннадий Волков, главный архитектор, и Лина Чуднова, руководитель направления Fast Data компании «Неофлекс».\n",
      "Пакетная обработка больших данных — это слишком медленно\n",
      "CNews: В экспертной среде достаточно много соображений о больших данных. Отдельные аналитики и представители бизнеса говорят, что они практически бесполезны, другие считают, что без больших данных компании достаточно быстро перестанут быть успешными в конкурентной борьбе. Какая у вас позиция на этот счет?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "for _ in range(5):\n",
    "    idx = randrange(len(filtered_docs)-1)\n",
    "    print(f'metadata: {filtered_docs[idx].metadata}')\n",
    "    print(f'{filtered_docs[idx].page_content}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiMseV6rr-wS"
   },
   "source": [
    "Загружаем все внутрь VectorStore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HzrSxT_ZfoZC"
   },
   "outputs": [],
   "source": [
    "document_ids = vector_store.add_documents(documents=filtered_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85qhokOsqS_B"
   },
   "source": [
    "Пишем будущий json запрос, передаем его POST запросом на сервер, получаем ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JvBJfyP8jwtz",
    "outputId": "4ec2b212-4396-4bf0-fbdf-b3c8b48758a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"answer\": \"Neoflex обладает экспертизой в области Site Reliability Engineering, DevOps, разработки Data-платформ, MLOps, построения аналитических платформ, Data Lake, проектирования моделей данных, построения ландшафтных архитектур, реализации трансформаций, построения BI, а также в управленческой и регуляторной отчётности.\",\n",
      "    \"source_documents\": [\n",
      "        {\n",
      "            \"source\": \"https://www.neoflex.ru/expertises/sre\",\n",
      "            \"snippet\": \"score: 0.192417648434639 passage: Neoflex — Экспертиза — Site Reliability Engineering\\nDevOps\\nSite Reliability Engineering\\nРазработка Data-платформ\\nТрансформация приложений, ин...\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"https://www.neoflex.ru/expertises/big-data\",\n",
      "            \"snippet\": \"score: 0.1954480826854706 passage: Neoflex — Экспертиза — Разработка Data-платформ\\nSite Reliability Engineering\\nРазработка Data-платформ\\nMLOps\\nСоздаем платформы для обработки и...\"\n",
      "        }\n",
      "    ],\n",
      "    \"session_id\": \"abc123\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    'session_id': 'abc123',\n",
    "    'question': 'В каких областях Neoflex обладает экспертизой?'\n",
    "}\n",
    "\n",
    "response = requests.post('http://127.0.0.1:8000/ask', json=query)\n",
    "print(json.dumps(response.json(), indent=4, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
